{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a069d7a4-e5de-4228-b4fe-8ebeb71a2f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑ million_dollar_bride/fulltext.docx...\n",
      "–ü—ã—Ç–∞—é—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª: million_dollar_bride/fulltext.docx\n",
      "–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç.\n",
      "–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á—ë–Ω.\n",
      "–†–∞–∑–¥–µ–ª—è—é —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã...\n",
      "–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª—ë–Ω –Ω–∞ 93 –≥–ª–∞–≤.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Clothes...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 277 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Clothes.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Hair...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 178 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Hair.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Appearances...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 129 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Appearances.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Weather...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 306 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Weather.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Locations...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 201 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Locations.\n",
      "–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Other...\n",
      "–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ 2104 –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Other.\n",
      "–§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω: million_dollar_bride/details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import docx\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∏—Å–µ–ª –≤ —á–∏—Å–ª–æ–≤—ã–µ\n",
    "TEXT_NUMBERS = {\n",
    "    'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6,\n",
    "    'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12,\n",
    "    'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16,\n",
    "    'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "    'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "    'eighty': 80, 'ninety': 90, 'hundred': 100\n",
    "}\n",
    "\n",
    "def word_to_number(word):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —á–∏—Å–ª–æ –≤ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.\"\"\"\n",
    "    words = re.split(r'[\\s\\-]+', word.lower())  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –¥–µ—Ñ–∏—Å–∞–º\n",
    "    number = 0\n",
    "    temp = 0\n",
    "\n",
    "    for w in words:\n",
    "        if w in TEXT_NUMBERS:\n",
    "            scale = TEXT_NUMBERS[w]\n",
    "            if scale == 100:  # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Ç–µ–Ω, –Ω–∞–ø—Ä–∏–º–µ—Ä \"one hundred\"\n",
    "                temp *= scale\n",
    "            else:\n",
    "                temp += scale\n",
    "        else:\n",
    "            return None  # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None\n",
    "\n",
    "    number += temp\n",
    "    return number\n",
    "\n",
    "def extract_chapter_number(text):\n",
    "    \"\"\"–ò—â–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"Chapter 1\"\n",
    "    numeric_match = re.match(r'Chapter\\s+(\\d+)', text, re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return int(numeric_match.group(1))\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"Chapter One\" –∏–ª–∏ \"Chapter Thirty-One\"\n",
    "    text_match = re.match(r'Chapter\\s+([\\w\\s\\-]+)', text, re.IGNORECASE)\n",
    "    if text_match:\n",
    "        word = text_match.group(1).strip()\n",
    "        return word_to_number(word)\n",
    "    return None\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ .docx —Ñ–∞–π–ª–∞.\"\"\"\n",
    "    try:\n",
    "        print(f\"–ü—ã—Ç–∞—é—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª: {file_path}\")\n",
    "        doc = docx.Document(file_path)\n",
    "        print(\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã—Ç.\")\n",
    "        full_text = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        print(\"–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á—ë–Ω.\")\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –∏–ª–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_text_into_chapters(text):\n",
    "    \"\"\"–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã.\"\"\"\n",
    "    if not text:\n",
    "        print(\"–¢–µ–∫—Å—Ç –ø—É—Å—Ç, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –≥–ª–∞–≤—ã.\")\n",
    "        return {}\n",
    "\n",
    "    print(\"–†–∞–∑–¥–µ–ª—è—é —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã...\")\n",
    "    chapters = defaultdict(tuple)\n",
    "    chapter_matches = list(re.finditer(r'Chapter\\s+([\\w\\s\\-]+)', text, re.IGNORECASE))\n",
    "\n",
    "    if not chapter_matches:\n",
    "        print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≥–ª–∞–≤—ã.\")\n",
    "        return {}\n",
    "\n",
    "    for i, match in enumerate(chapter_matches):\n",
    "        start = match.start()\n",
    "        chapter_title = match.group(0)\n",
    "        chapter_number = extract_chapter_number(chapter_title)  # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã\n",
    "        end = chapter_matches[i + 1].start() if i + 1 < len(chapter_matches) else len(text)\n",
    "        chapter_text = text[start:end].strip()\n",
    "        if chapter_number is not None:\n",
    "            chapters[chapter_number] = (chapter_title, chapter_text)\n",
    "    \n",
    "    print(f\"–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª—ë–Ω –Ω–∞ {len(chapters)} –≥–ª–∞–≤.\")\n",
    "    return chapters\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–π –≥–ª–∞–≤–µ\n",
    "def search_words_in_chapter(chapter_text, words):\n",
    "    word_pattern = r'\\b(' + '|'.join(re.escape(word) for word in words) + r')\\b'\n",
    "    \n",
    "    matches = []\n",
    "    for match in re.finditer(word_pattern, chapter_text, re.IGNORECASE):\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–æ —Ç—Ä–∏ —Å–ª–æ–≤–∞ –¥–æ –∏ –ø–æ—Å–ª–µ\n",
    "        start_context = chapter_text[:start].split()[-8:]\n",
    "        end_context = chapter_text[end:].split()[:8]\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∏ –ø–æ–∑–∏—Ü–∏—é –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "        found_word = chapter_text[start:end]\n",
    "        result = ' '.join(start_context + [found_word] + end_context)\n",
    "        \n",
    "        matches.append((start, result))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "    matches.sort(key=lambda x: x[0])\n",
    "    \n",
    "    return [match[1] for match in matches]\n",
    "    \n",
    "def search_in_all_chapters(chapters, words, category):\n",
    "    if not chapters:\n",
    "        print(\"–ù–µ—Ç –≥–ª–∞–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"–ò—â—É –∑–∞–¥–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}...\")\n",
    "    results = []\n",
    "    for chapter_number, (chapter_title, chapter_text) in chapters.items():\n",
    "        matches = search_words_in_chapter(chapter_text, words)\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                results.append([chapter_number, chapter_title, match, category])\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ {len(results)} –≥–ª–∞–≤–∞—Ö –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}.\")\n",
    "    return results\n",
    "\n",
    "# –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∫–æ–¥–∞ –æ—Å—Ç–∞—ë—Ç—Å—è –ø—Ä–µ–∂–Ω–µ–π...\n",
    "\n",
    "def main(docx_file_path, chapters_folder):\n",
    "    if not os.path.exists(docx_file_path):\n",
    "        print(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {docx_file_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"–ò–∑–≤–ª–µ–∫–∞—é —Ç–µ–∫—Å—Ç –∏–∑ {docx_file_path}...\")\n",
    "    book_text = extract_text_from_docx(docx_file_path)\n",
    "    \n",
    "    if not book_text:\n",
    "        print(\"–û—à–∏–±–∫–∞: —Ç–µ–∫—Å—Ç –Ω–µ –±—ã–ª –∏–∑–≤–ª–µ—á—ë–Ω.\")\n",
    "        return\n",
    "\n",
    "    chapters = split_text_into_chapters(book_text)\n",
    "    \n",
    "    if not chapters:\n",
    "        print(\"–û—à–∏–±–∫–∞: –≥–ª–∞–≤—ã –Ω–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã.\")\n",
    "        return\n",
    "\n",
    "    clothes_words = ['stilettos', 'sunglasses', 'ring', 'necklace', 'bracelet', 'earrings', 'brooch', 'watch', 'anklet', 'choker', 'pendant', 'cufflinks', 'tie clip', 'nose ring', 'belly button ring', 'toe ring', 'hairpin', 'tiara', 'diadem', 'bangle', 'chain', 'medallion', 'pearl necklace', 'locket', 'armband', 'charm bracelet', 'dress', 'robe', 'suit', 'clothes', 'coat', 'jacket', 'shirt', 'pants', 'skirt', 'jeans', 't-shirt', 'sweater', 'blouse', 'shorts', 'hoodie', 'vest', 'scarf', 'hat', 'gloves', 'boots', 'shoes', 'sneakers', 'socks', 'tie', 'belt', 'gown', 'trench coat', 'blazer', 'cardigan', 'overalls', 'tank top', 'leggings']\n",
    "    hair_words = ['hair', 'beard', 'ponytail', 'bun', 'braids', 'bob', 'pixie cut', 'long waves', 'curly hair', 'straight hair', 'afro', 'buzz cut', 'french twist', 'dreadlocks', 'fishtail braid', 'half-up half-down', 'side part', 'middle part', 'updo', 'loose curls', 'locks', 'layered cut', 'shag cut', 'crew cut', 'mohawk', 'bangs', 'chignon', 'top knot']\n",
    "    appearances_words = ['fur', 'black wolf', 'white wolf', 'brown wolf', 'caramel hair', 'blonde', 'brunette', 'redhead', 'white hair', 'red hair', 'auburn hair', 'chestnut hair', 'black hair', 'grey hair', 'dark hair', 'blue eyes', 'blue irises', 'blue eyeballs', 'brown eyes', 'brown irises', 'brown eyeballs', 'black eyes', 'black irises', 'black eyeballs', 'red eyes', 'red irises', 'red eyeballs', 'hazel eyes', 'hazel irises', 'hazel eyeballs', 'green eyes', 'green irises', 'green eyeballs', 'eyes were green', 'eyes were black', 'eyes were brown', 'eyes were hazel', 'eyes were blue', 'eyes were grey', '5 feet', \"5'\", \"6'\", \"7'\", '6 feet', '7 feet', 'feet tall', 'slim', 'thin', 'thick', 'tall', 'dark skin', 'white skin', 'pale skin', 'freckles', 'tattoos', 'tattoo', 'brown skin', 'black skin', 'high cheekbones', 'wrinkles', 'wrinkled', 'full lips', 'small breasts', 'curves', 'big breasts']\n",
    "    other_words = ['eyes', 'face', 'skin', 'body', 'fur', 'chin', 'cheeks', 'big wolf', 'white wolf', 'beautiful', 'ugly', 'handsome', 'cute', 'gorgeous', 'sharp', 'features', 'arms', 'bicep', 'legs', 'ass', 'breasts', 'waist', 'muscular', 'pale', 'features', 'thin', 'fangs', 'tattoos', 'teeth', 'mouth', 'young', 'old', 'blood', 'bleeding', 'gaze', 'smirk', 'smile', 'lips', 'nose', 'hands', 'jaw']\n",
    "    weather_words = ['morning', 'afternoon', 'evening', 'night', 'sunrise', 'sunset', 'dawn', 'dusk', 'noon', 'midnight', 'cloudy', 'rain', 'storm', 'wind', 'sun', 'sunny', 'fog', 'foggy', 'snow', 'snowy', 'hail', 'thunder', 'lightning', 'breeze', 'chilly', 'hot', 'warm', 'cold', 'frost', 'blizzard', 'temperature', 'humid', 'dry', 'drizzle', 'pouring', 'downpour', 'mist', 'overcast']\n",
    "    locations_words = ['forest', 'living room', 'dining room', 'school', 'college', 'training grounds', 'field', 'bathroom', 'bedroom', 'cabin', 'house']\n",
    "   \n",
    "    clothes_results = search_in_all_chapters(chapters, clothes_words, 'Clothes')\n",
    "    hair_results = search_in_all_chapters(chapters, hair_words, 'Hair')\n",
    "    appearances_results = search_in_all_chapters(chapters, appearances_words, 'Appearances')\n",
    "    weather_results = search_in_all_chapters(chapters, weather_words, 'Weather')\n",
    "    locations_results = search_in_all_chapters(chapters, locations_words, 'Locations')\n",
    "    other_results = search_in_all_chapters(chapters, other_words, 'Other')\n",
    "\n",
    "    all_results = clothes_results + hair_results + appearances_results + weather_results + locations_results + other_results\n",
    "\n",
    "    all_data_df = pd.DataFrame(all_results, columns=['Chapter Number', 'Chapter Title', 'Match', 'Category'])\n",
    "    all_data_df.sort_values(by=['Chapter Number'], inplace=True)\n",
    "\n",
    "    os.makedirs(f\"{chapters_folder}\", exist_ok=True)\n",
    "\n",
    "    excel_file_path = f\"{chapters_folder}/details.xlsx\"\n",
    "    try:\n",
    "        all_data_df.to_excel(excel_file_path, sheet_name='Details', index=False)\n",
    "        print(f\"–§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω: {excel_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞: {e}\")\n",
    "\n",
    "# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "chapters_folder = \"million_dollar_bride\"\n",
    "docx_file_path = f\"{chapters_folder}/fulltext.docx\"\n",
    "main(docx_file_path, chapters_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74640a06-de13-4445-ac48-c7658ae48e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: million_dollar_bride/fulltext.docx\n",
      "üìñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞...\n",
      "‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≥–ª–∞–≤—ã...\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Clothes': –Ω–∞–π–¥–µ–Ω–æ 277 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Hair': –Ω–∞–π–¥–µ–Ω–æ 178 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Appearances': –Ω–∞–π–¥–µ–Ω–æ 129 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Weather': –Ω–∞–π–¥–µ–Ω–æ 306 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Locations': –Ω–∞–π–¥–µ–Ω–æ 201 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è 'Other': –Ω–∞–π–¥–µ–Ω–æ 2104 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\n",
      "üìä –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏...\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel...\n",
      "‚úÖ –§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω: million_dollar_bride/details.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def search_words_in_chapter(chapter_text, words):\n",
    "    \"\"\"–ò—â–µ—Ç —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ –≥–ª–∞–≤—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö —Å –ø–æ–∑–∏—Ü–∏—è–º–∏.\"\"\"\n",
    "    word_pattern = r'\\b(' + '|'.join(re.escape(word) for word in words) + r')\\b'\n",
    "    matches = []\n",
    "\n",
    "    for match in re.finditer(word_pattern, chapter_text, re.IGNORECASE):\n",
    "        start, end = match.start(), match.end()\n",
    "\n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "        start_context = chapter_text[:start].split()[-8:]\n",
    "        end_context = chapter_text[end:].split()[:8]\n",
    "        found_word = chapter_text[start:end]\n",
    "        result = ' '.join(start_context + [found_word] + end_context)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        matches.append((start, result))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def search_in_all_chapters(chapters, words, category):\n",
    "    \"\"\"–ò—â–µ—Ç —Å–ª–æ–≤–∞ –≤–æ –≤—Å–µ—Ö –≥–ª–∞–≤–∞—Ö –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\"\"\"\n",
    "    total_matches = 0\n",
    "    results = []\n",
    "\n",
    "    for chapter_number, chapter_title, chapter_text in chapters:\n",
    "        matches = search_words_in_chapter(chapter_text, words)\n",
    "        total_matches += len(matches)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "\n",
    "        for position, match in matches:\n",
    "            results.append([chapter_number, chapter_title, position, match, category])\n",
    "\n",
    "    print(f\"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{category}': –Ω–∞–π–¥–µ–Ω–æ {total_matches} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\")\n",
    "    return results\n",
    "\n",
    "def main(docx_file_path, chapters_folder):\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–∏—Å–∫–∞.\"\"\"\n",
    "    print(f\"üìÇ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {docx_file_path}\")\n",
    "    if not os.path.exists(docx_file_path):\n",
    "        print(f\"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {docx_file_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"üìñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞...\")\n",
    "    book_text = extract_text_from_docx(docx_file_path)\n",
    "    if not book_text:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≥–ª–∞–≤—ã...\")\n",
    "    chapters = split_text_into_chapters(book_text)\n",
    "    if not chapters:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã.\")\n",
    "        return\n",
    "\n",
    "    # –°–ø–∏—Å–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "    clothes_words = ['stilettos', 'sunglasses', 'ring', 'necklace', 'bracelet', 'earrings', 'brooch', 'watch', 'anklet', 'choker', 'pendant', 'cufflinks', 'tie clip', 'nose ring', 'belly button ring', 'toe ring', 'hairpin', 'tiara', 'diadem', 'bangle', 'chain', 'medallion', 'pearl necklace', 'locket', 'armband', 'charm bracelet', 'dress', 'robe', 'suit', 'clothes', 'coat', 'jacket', 'shirt', 'pants', 'skirt', 'jeans', 't-shirt', 'sweater', 'blouse', 'shorts', 'hoodie', 'vest', 'scarf', 'hat', 'gloves', 'boots', 'shoes', 'sneakers', 'socks', 'tie', 'belt', 'gown', 'trench coat', 'blazer', 'cardigan', 'overalls', 'tank top', 'leggings']\n",
    "    hair_words = ['hair', 'beard', 'ponytail', 'bun', 'braids', 'bob', 'pixie cut', 'long waves', 'curly hair', 'straight hair', 'afro', 'buzz cut', 'french twist', 'dreadlocks', 'fishtail braid', 'half-up half-down', 'side part', 'middle part', 'updo', 'loose curls', 'locks', 'layered cut', 'shag cut', 'crew cut', 'mohawk', 'bangs', 'chignon', 'top knot']\n",
    "    appearances_words = ['fur', 'black wolf', 'white wolf', 'brown wolf', 'caramel hair', 'blonde', 'brunette', 'redhead', 'white hair', 'red hair', 'auburn hair', 'chestnut hair', 'black hair', 'grey hair', 'dark hair', 'blue eyes', 'blue irises', 'blue eyeballs', 'brown eyes', 'brown irises', 'brown eyeballs', 'black eyes', 'black irises', 'black eyeballs', 'red eyes', 'red irises', 'red eyeballs', 'hazel eyes', 'hazel irises', 'hazel eyeballs', 'green eyes', 'green irises', 'green eyeballs', 'eyes were green', 'eyes were black', 'eyes were brown', 'eyes were hazel', 'eyes were blue', 'eyes were grey', '5 feet', \"5'\", \"6'\", \"7'\", '6 feet', '7 feet', 'feet tall', 'slim', 'thin', 'thick', 'tall', 'dark skin', 'white skin', 'pale skin', 'freckles', 'tattoos', 'tattoo', 'brown skin', 'black skin', 'high cheekbones', 'wrinkles', 'wrinkled', 'full lips', 'small breasts', 'curves', 'big breasts']\n",
    "    other_words = ['eyes', 'face', 'skin', 'body', 'fur', 'chin', 'cheeks', 'big wolf', 'white wolf', 'beautiful', 'ugly', 'handsome', 'cute', 'gorgeous', 'sharp', 'features', 'arms', 'bicep', 'legs', 'ass', 'breasts', 'waist', 'muscular', 'pale', 'features', 'thin', 'fangs', 'tattoos', 'teeth', 'mouth', 'young', 'old', 'blood', 'bleeding', 'gaze', 'smirk', 'smile', 'lips', 'nose', 'hands', 'jaw']\n",
    "    weather_words = ['morning', 'afternoon', 'evening', 'night', 'sunrise', 'sunset', 'dawn', 'dusk', 'noon', 'midnight', 'cloudy', 'rain', 'storm', 'wind', 'sun', 'sunny', 'fog', 'foggy', 'snow', 'snowy', 'hail', 'thunder', 'lightning', 'breeze', 'chilly', 'hot', 'warm', 'cold', 'frost', 'blizzard', 'temperature', 'humid', 'dry', 'drizzle', 'pouring', 'downpour', 'mist', 'overcast']\n",
    "    locations_words = ['forest', 'living room', 'dining room', 'school', 'college', 'training grounds', 'field', 'bathroom', 'bedroom', 'cabin', 'house']\n",
    "   \n",
    "\n",
    "    all_results = []\n",
    "    for words, category in [\n",
    "        (clothes_words, 'Clothes'), (hair_words, 'Hair'),\n",
    "        (appearances_words, 'Appearances'), (weather_words, 'Weather'),\n",
    "        (locations_words, 'Locations'), (other_words, 'Other')\n",
    "    ]:\n",
    "        all_results.extend(search_in_all_chapters(chapters, words, category))\n",
    "\n",
    "    print(\"üìä –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏...\")\n",
    "    all_data_df = pd.DataFrame(all_results, columns=['Chapter Number', 'Chapter Title', 'Position', 'Match', 'Category'])\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É –≥–ª–∞–≤—ã –∏ –ø–æ–∑–∏—Ü–∏–∏\n",
    "    all_data_df.sort_values(by=['Chapter Number', 'Position'], inplace=True)\n",
    "\n",
    "    print(\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel...\")\n",
    "    os.makedirs(chapters_folder, exist_ok=True)\n",
    "    excel_file_path = f\"{chapters_folder}/details.xlsx\"\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫—É 'Position' –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º\n",
    "    try:\n",
    "        all_data_df.drop(columns=['Position']).to_excel(excel_file_path, sheet_name='Details', index=False)\n",
    "        print(f\"‚úÖ –§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω: {excel_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞: {e}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞\n",
    "chapters_folder = \"million_dollar_bride\"\n",
    "docx_file_path = f\"{chapters_folder}/fulltext.docx\"\n",
    "main(docx_file_path, chapters_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31092458-80fc-4213-9f4d-125f26343469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
