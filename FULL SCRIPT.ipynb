{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e426adf8-4904-4ff5-80ce-a5a576781ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f445c00d034a089758ff067bbe6474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.docx', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4640ad83ea4340fbb0351b06e1859776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Check', style=ButtonStyle(), tooltip='Check')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489c81cbd5c74813ae38c20f02962a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Split', style=ButtonStyle(), tooltip='Split')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3e888dd7cb43febf78a877493be40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Find Appearance', style=ButtonStyle(), tooltip='Find Appearance')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2244e027f77d404dadbc191edf2c9abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clean summary', style=ButtonStyle(), tooltip='Clean summary')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceae33ad18e4df8972eaffd3b74e83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Create cover file', style=ButtonStyle(), tooltip='Create cover file')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddb34d359804e17a34f5abed9fe4851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##PROMPT##\n",
    "\n",
    "##–ü—Ä–æ—á–∏—Ç–∞–π —Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤—ã –∏ —Å–¥–µ–ª–∞–π –¥–ª—è –Ω–µ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º. \n",
    "##–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –≤–∫–ª—é—á–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã; —Å–æ–±—ã—Ç–∏—è –≥–ª–∞–≤—ã –≤ 2-3 –∞–±–∑–∞—Ü–∞—Ö (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –°–æ–±—ã—Ç–∏—è –≥–ª–∞–≤—ã); \n",
    "##—Å–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, —É—á–∞—Å—Ç–≤—É—é—â–∏—Ö –≤ –≥–ª–∞–≤–µ, —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏—Ö –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ - –≤–æ–∑—Ä–∞—Å—Ç, –∫–æ–∂–∞, –≤–æ–ª–æ—Å—ã, –≥–ª–∞–∑–∞, —Ä–æ—Å—Ç, –æ–¥–µ–∂–¥–∞, –ø–æ–∑–∞ –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ (–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –≥–ª–∞–≤—ã); \n",
    "##—Å–ø–∏—Å–æ–∫ –ª–æ–∫–∞—Ü–∏–π –≥–ª–∞–≤—ã –∏ –∏—Ö –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ (–õ–æ–∫–∞—Ü–∏–∏ –≥–ª–∞–≤—ã); –ø–æ–≥–æ–¥–∞ –∏ –≤—Ä–µ–º—è –¥–Ω—è, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ (–ü–æ–≥–æ–¥–∞ –∏ –≤—Ä–µ–º—è –¥–Ω—è). –ò–∑–±–µ–≥–∞–π –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π. Use temperature = 0.3##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import pythoncom\n",
    "import math\n",
    "from win32com import client as win32\n",
    "\n",
    "from IPython.display import display\n",
    "from docx import Document\n",
    "from collections import defaultdict\n",
    "from words import (\n",
    "    clothes_words, hair_words, appearances_words, weather_words,\n",
    "    locations_words, age_words, other_words\n",
    ")\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –≤–∏–¥–∂–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept = '.docx',  # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .docx\n",
    "    multiple = False  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ñ–∞–π–ª\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–Ω–æ–ø–∫–∏\n",
    "analyze_button = widgets.Button(\n",
    "    description = 'Check',\n",
    "    disabled = False,\n",
    "    button_style = '',  # 'success', 'info', 'warning', 'danger' –∏–ª–∏ ''\n",
    "    tooltip = 'Check',\n",
    "    icon = ''  # –ò–∫–æ–Ω–∫–∞ (–∏–º—è FontAwesome –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `fa-`)\n",
    ")\n",
    "\n",
    "split_by_chapters_button = widgets.Button(\n",
    "    description = 'Split',\n",
    "    disabled = False,\n",
    "    button_style = '',  # 'success', 'info', 'warning', 'danger' –∏–ª–∏ ''\n",
    "    tooltip = 'Split',\n",
    "    icon = ''  # –ò–∫–æ–Ω–∫–∞ (–∏–º—è FontAwesome –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `fa-`)\n",
    ")\n",
    "\n",
    "find_appearance_button = widgets.Button(\n",
    "    description = 'Find Appearance',\n",
    "    disabled = False,\n",
    "    button_style = '',  # 'success', 'info', 'warning', 'danger' –∏–ª–∏ ''\n",
    "    tooltip = 'Find Appearance',\n",
    "    icon = ''  # –ò–∫–æ–Ω–∫–∞ (–∏–º—è FontAwesome –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `fa-`)\n",
    ")\n",
    "\n",
    "clean_summary_button = widgets.Button(\n",
    "    description = 'Clean summary',\n",
    "    disabled = False,\n",
    "    button_style = '',  # 'success', 'info', 'warning', 'danger' –∏–ª–∏ ''\n",
    "    tooltip = 'Clean summary',\n",
    "    icon = ''  # –ò–∫–æ–Ω–∫–∞ (–∏–º—è FontAwesome –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `fa-`)\n",
    ")\n",
    "\n",
    "cover_info_button = widgets.Button(\n",
    "    description = 'Create cover file',\n",
    "    disabled = False,\n",
    "    button_style = '',  # 'success', 'info', 'warning', 'danger' –∏–ª–∏ ''\n",
    "    tooltip = 'Create cover file',\n",
    "    icon = ''  # –ò–∫–æ–Ω–∫–∞ (–∏–º—è FontAwesome –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `fa-`)\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –≤–∏–¥–∂–µ—Ç –≤—ã–≤–æ–¥–∞\n",
    "output = widgets.Output()\n",
    "\n",
    "\n",
    "def convert_docm_to_docx(input_path, output_path):\n",
    "    \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–∞–π–ª .docm –≤ .docx —Å –ø–æ–º–æ—â—å—é Microsoft Word.\"\"\"\n",
    "    try:\n",
    "        pythoncom.CoInitialize()\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # –ó–∞–ø—É—Å–∫ Word –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ\n",
    "\n",
    "        print(f\"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {input_path} –≤ {output_path}...\")\n",
    "        doc = word.Documents.Open(input_path)\n",
    "        doc.SaveAs(output_path, FileFormat=16)  # 16 ‚Äî —ç—Ç–æ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è .docx\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        print(f\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        pythoncom.CoUninitialize()\n",
    "\n",
    "\n",
    "def is_valid_docx(file_content):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç Word.\"\"\"\n",
    "    try:\n",
    "        docx_file = io.BytesIO(file_content)\n",
    "        Document(docx_file)  # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–∫ .docx\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_docx_file(file_info):\n",
    "    try:\n",
    "        filename = file_info['name']\n",
    "        file_content = file_info['content']\n",
    "\n",
    "        if not is_valid_docx(file_content):\n",
    "            print(f\"–û—à–∏–±–∫–∞: –§–∞–π–ª {filename} –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º .docx/.docm.\")\n",
    "            return None  # –î–æ–±–∞–≤–ª—è–µ–º return –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª .docm\n",
    "        if filename.endswith('.docm'):\n",
    "            print(\"–û–±–Ω–∞—Ä—É–∂–µ–Ω .docm —Ñ–∞–π–ª, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è...\")\n",
    "            temp_docm_path = \"temp_file.docm\"\n",
    "            with open(temp_docm_path, 'wb') as f:\n",
    "                f.write(file_content)\n",
    "\n",
    "            temp_docx_path = \"temp_file.docx\"\n",
    "            if convert_docm_to_docx(temp_docm_path, temp_docx_path):\n",
    "                print(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ .docx —Ñ–∞–π–ª–∞...\")\n",
    "                with open(temp_docx_path, 'rb') as f:\n",
    "                    docx_file = io.BytesIO(f.read())\n",
    "\n",
    "                # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\n",
    "                os.remove(temp_docm_path)\n",
    "                os.remove(temp_docx_path)\n",
    "            else:\n",
    "                print(\"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ñ–∞–π–ª .docx...\")\n",
    "            docx_file = io.BytesIO(file_content)\n",
    "\n",
    "        # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–∫—Ä—ã—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç Word\n",
    "        docx = Document(docx_file)\n",
    "        print(\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø—Ä–æ—á–∏—Ç–∞–Ω.\")\n",
    "        return docx\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–æ–∫\n",
    "def on_analyze_button_click(event):\n",
    "    with output:\n",
    "        output.clear_output()  # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—ã–≤–æ–¥\n",
    "        if upload_widget.value:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ)\n",
    "            file_info = list(upload_widget.value)[0]\n",
    "            filename = file_info['name']  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            print(f'–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {filename}')\n",
    "            \n",
    "            docx_doc = load_docx_file(file_info)\n",
    "            analyze_book(docx_doc)\n",
    "        else:\n",
    "            print(\"–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "def on_split_by_chapters_button_click(event):\n",
    "    with output:\n",
    "        output.clear_output()  # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—ã–≤–æ–¥\n",
    "        if upload_widget.value:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ)\n",
    "            file_info = list(upload_widget.value)[0]\n",
    "            filename = file_info['name']  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            \n",
    "            print(f'–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {filename}')\n",
    "\n",
    "            path = os.path.splitext(filename)[0]\n",
    "            docx_doc = load_docx_file(file_info)\n",
    "            \n",
    "            split_chapters(docx_doc, path)\n",
    "        else:\n",
    "            print(\"–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "def on_find_appearance_button_click(event):\n",
    "    with output:\n",
    "        output.clear_output()  # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—ã–≤–æ–¥\n",
    "        if upload_widget.value:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ)\n",
    "            file_info = list(upload_widget.value)[0]\n",
    "            filename = file_info['name']  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            \n",
    "            print(f'–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {filename}')\n",
    "\n",
    "            path = os.path.splitext(filename)[0]\n",
    "            docx_doc = load_docx_file(file_info)\n",
    "            \n",
    "            find_appearance(docx_doc, path)\n",
    "        else:\n",
    "            print(\"–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "def on_clean_summary_button_click(event):\n",
    "    with output:\n",
    "        output.clear_output()  # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—ã–≤–æ–¥\n",
    "        if upload_widget.value:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ)\n",
    "            file_info = list(upload_widget.value)[0]\n",
    "            filename = file_info['name']  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            \n",
    "            print(f'–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {filename}')\n",
    "\n",
    "            path = os.path.splitext(filename)[0]\n",
    "            docx_doc = load_docx_file(file_info)\n",
    "            \n",
    "            clean_summary(docx_doc, path)\n",
    "        else:\n",
    "            print(\"–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "def on_cover_info_button_click(event):\n",
    "    with output:\n",
    "        output.clear_output()  # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—ã–≤–æ–¥\n",
    "        if upload_widget.value:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ)\n",
    "            file_info = list(upload_widget.value)[0]\n",
    "            filename = file_info['name']  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "            \n",
    "            print(f'–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {filename}')\n",
    "\n",
    "            path = os.path.splitext(filename)[0]\n",
    "            docx_doc = load_docx_file(file_info)\n",
    "            \n",
    "            create_cover_info(docx_doc, path)\n",
    "        else:\n",
    "            print(\"–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "            \n",
    "\n",
    "# –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –∫–Ω–æ–ø–∫–µ\n",
    "analyze_button.on_click(on_analyze_button_click)\n",
    "split_by_chapters_button.on_click(on_split_by_chapters_button_click)\n",
    "find_appearance_button.on_click(on_find_appearance_button_click)\n",
    "clean_summary_button.on_click(on_clean_summary_button_click)\n",
    "cover_info_button.on_click(on_cover_info_button_click)\n",
    "\n",
    "# –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–∏–¥–∂–µ—Ç—ã\n",
    "display(upload_widget, analyze_button, split_by_chapters_button, find_appearance_button, clean_summary_button, cover_info_button, output)\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —á–∏—Å–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞\n",
    "TEXT_NUMBERS = {\n",
    "    'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6,\n",
    "    'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10, 'eleven': 11, 'twelve': 12,\n",
    "    'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16,\n",
    "    'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "    'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "    'eighty': 80, 'ninety': 90, 'hundred': 100\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "### 00_book_Check ###\n",
    "\n",
    "def word_to_number(word):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —á–∏—Å–ª–æ –≤ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.\"\"\"\n",
    "    words = re.split(r'[\\s\\-]+', word.lower())  # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –¥–µ—Ñ–∏—Å–∞–º\n",
    "    number = 0\n",
    "    temp = 0\n",
    "\n",
    "    for w in words:\n",
    "        if w in TEXT_NUMBERS:\n",
    "            scale = TEXT_NUMBERS[w]\n",
    "            if scale == 100:  # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Ç–µ–Ω, –Ω–∞–ø—Ä–∏–º–µ—Ä \"one hundred\"\n",
    "                temp *= scale\n",
    "            else:\n",
    "                temp += scale\n",
    "        else:\n",
    "            return None  # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None\n",
    "\n",
    "    number += temp\n",
    "    return number\n",
    "\n",
    "def extract_chapter_number(text):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"Chapter 1\" –∏–ª–∏ \"Chapter 1 - School Trouble\"\n",
    "    numeric_match = re.match(r'Chapter\\s+(\\d+)', text, re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return int(numeric_match.group(1))\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"Chapter One\" –∏–ª–∏ \"Chapter Thirty-one\"\n",
    "    text_match = re.match(r'Chapter\\s+([\\w\\s\\-]+)', text, re.IGNORECASE)\n",
    "    if text_match:\n",
    "        word = text_match.group(1).strip()\n",
    "        return word_to_number(word)\n",
    "    return None\n",
    "\n",
    "def extract_chapters(doc):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤—ã –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ .docx —Ñ–∞–π–ª–∞.\"\"\"\n",
    "    chapters = []\n",
    "    current_chapter = None\n",
    "    current_text = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        chapter_num = extract_chapter_number(para.text.strip())\n",
    "        if chapter_num is not None:\n",
    "            if current_chapter is not None and current_text:\n",
    "                chapters.append((current_chapter, \"\\n\".join(current_text).strip()))\n",
    "            current_chapter = chapter_num\n",
    "            current_text = []\n",
    "        else:\n",
    "            current_text.append(para.text.strip())\n",
    "\n",
    "    if current_chapter is not None and current_text:\n",
    "        chapters.append((current_chapter, \"\\n\".join(current_text).strip()))\n",
    "\n",
    "    return chapters\n",
    "\n",
    "def check_duplicate_chapters(chapters):\n",
    "    chapter_numbers = [chap[0] for chap in chapters]\n",
    "    return {chap for chap in chapter_numbers if chapter_numbers.count(chap) > 1}\n",
    "\n",
    "def check_text_duplicates(chapters):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è —Ç–µ–∫—Å—Ç –≤ –≥–ª–∞–≤–∞—Ö —Å —É—á—ë—Ç–æ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\"\"\"\n",
    "    text_to_chapter = defaultdict(list)\n",
    "\n",
    "    for chapter, text in chapters:\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        normalized_text = \" \".join(text.split()).lower()\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã –≤ —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≤ —Å —Ç–∞–∫–∏–º –∂–µ —Ç–µ–∫—Å—Ç–æ–º\n",
    "        text_to_chapter[normalized_text].append(chapter)\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∑–∞–ø–∏—Å–∏, –≥–¥–µ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ç–µ–∫—Å—Ç –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–∞\n",
    "    return {text: chaps for text, chaps in text_to_chapter.items() if len(chaps) > 1}\n",
    "\n",
    "def check_chapter_order(chapters):\n",
    "    chapter_numbers = [chap[0] for chap in chapters]\n",
    "    return [(chapter_numbers[i], chapter_numbers[i+1]) \n",
    "            for i in range(len(chapter_numbers) - 1) if chapter_numbers[i] > chapter_numbers[i + 1]]\n",
    "\n",
    "def check_missing_chapters(chapters):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\"\"\"\n",
    "    chapter_numbers = sorted([chap[0] for chap in chapters])\n",
    "    missing_chapters = [num for num in range(chapter_numbers[0], chapter_numbers[-1] + 1) \n",
    "                        if num not in chapter_numbers]\n",
    "    return missing_chapters\n",
    "\n",
    "def analyze_book(docx_file):\n",
    "    chapters = extract_chapters(docx_file)\n",
    "\n",
    "    print(\"–°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤:\")\n",
    "    for chapter, text in chapters:\n",
    "        print(f\"Chapter {chapter}: {text[:50]}...\")\n",
    "    \n",
    "    duplicate_chapters = check_duplicate_chapters(chapters)\n",
    "    duplicate_texts = check_text_duplicates(chapters)\n",
    "    unordered_chapters = check_chapter_order(chapters)\n",
    "    missing_chapters = check_missing_chapters(chapters)\n",
    "\n",
    "    print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–æ–∫:\")\n",
    "    print(f\"‚ùå –ù–∞–π–¥–µ–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –≥–ª–∞–≤—ã: {duplicate_chapters}\" if duplicate_chapters else \"‚úÖ –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\")\n",
    "    print(f\"‚ùå –ù–∞–π–¥–µ–Ω—ã –≥–ª–∞–≤—ã —Å –¥—É–±–ª–∏—Ä—É—é—â–∏–º—Å—è —Ç–µ–∫—Å—Ç–æ–º: {duplicate_texts}\" if duplicate_texts else \"‚úÖ –î—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.\")\n",
    "    print(f\"‚ùå –ù–∞–π–¥–µ–Ω—ã –≥–ª–∞–≤—ã —Å –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞: {unordered_chapters}\" if unordered_chapters else \"‚úÖ –ù–∞—Ä—É—à–µ–Ω–∏–π –≤ –ø–æ—Ä—è–¥–∫–µ –≥–ª–∞–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\")\n",
    "    print(f\"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã: {missing_chapters}\" if missing_chapters else \"‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
    "\n",
    "\n",
    "\n",
    "### 01_Breaking_into_chapters ###\n",
    "\n",
    "def split_chapters(doc, path):\n",
    "    #print('split_chapters(doc_path in doc.paragraph)')\n",
    "    # –ë—É—Ñ–µ—Ä –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã\n",
    "    current_chapter = []\n",
    "    chapter_number = 0\n",
    "\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤, –≤–∫–ª—é—á–∞—è –¥–µ—Ñ–∏—Å\n",
    "    chapter_pattern = re.compile('Chapter', re.IGNORECASE)\n",
    "    found_first_chapter = False  # –§–ª–∞–≥ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –ø–µ—Ä–≤–æ–π –≥–ª–∞–≤—ã\n",
    "\n",
    "    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    for paragraph in doc.paragraphs:\n",
    "        # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫—É –≥–ª–∞–≤—ã\n",
    "        if chapter_pattern.match(paragraph.text.strip()):\n",
    "            if found_first_chapter and current_chapter:\n",
    "                save_chapter(current_chapter, chapter_number, path)\n",
    "\n",
    "            # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è –Ω–æ–≤–æ–π –≥–ª–∞–≤—ã –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã\n",
    "            current_chapter = [paragraph.text]\n",
    "            chapter_number += 1\n",
    "            found_first_chapter = True  # –ü–µ—Ä–≤–∞—è –≥–ª–∞–≤–∞ –Ω–∞–π–¥–µ–Ω–∞\n",
    "        else:\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –≤ —Ç–µ–∫—É—â—É—é –≥–ª–∞–≤—É, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –≥–ª–∞–≤–∞ —É–∂–µ –Ω–∞–π–¥–µ–Ω–∞\n",
    "            if found_first_chapter:\n",
    "                current_chapter.append(paragraph.text)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞–≤—É\n",
    "    if current_chapter:\n",
    "        save_chapter(current_chapter, chapter_number, path)\n",
    "\n",
    "def save_chapter(chapter_content, chapter_number, path):\n",
    "    print('save')\n",
    "    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –≥–ª–∞–≤—ã\n",
    "    new_doc = Document()\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã –≤ –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "    for paragraph in chapter_content:\n",
    "        new_doc.add_paragraph(paragraph)\n",
    "    \n",
    "    # –£–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "    save_directory = f'{path}'  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –Ω—É–∂–Ω—ã–π –ø—É—Ç—å\n",
    "    \n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞–µ–º\n",
    "    os.makedirs(save_directory, exist_ok = True)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º Chapter_N.docx –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –ø–∞–ø–∫—É\n",
    "    chapter_filename = os.path.join(save_directory, f'Chapter_{chapter_number}.docx')\n",
    "    new_doc.save(chapter_filename)\n",
    "    print(f'Chapter {chapter_number} saved as {chapter_filename}')\n",
    "\n",
    "\n",
    "\n",
    "### 02_looking_for_appearance ###\n",
    "\n",
    "def search_words_in_chapter(chapter_text, words):\n",
    "    \"\"\"–ò—â–µ—Ç —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ –≥–ª–∞–≤—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö —Å –ø–æ–∑–∏—Ü–∏—è–º–∏.\"\"\"\n",
    "    word_pattern = r'\\b(' + '|'.join(re.escape(word) for word in words) + r')\\b'\n",
    "    matches = []\n",
    "\n",
    "    for match in re.finditer(word_pattern, chapter_text, re.IGNORECASE):\n",
    "        start, end = match.start(), match.end()\n",
    "\n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞\n",
    "        start_context = chapter_text[:start].split()[-8:]\n",
    "        end_context = chapter_text[end:].split()[:8]\n",
    "        found_word = chapter_text[start:end]\n",
    "        result = ' '.join(start_context + [found_word] + end_context)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–∑–∏—Ü–∏—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        matches.append((start, result))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def search_in_all_chapters(chapters, words, category):\n",
    "    \"\"\"–ò—â–µ—Ç —Å–ª–æ–≤–∞ –≤–æ –≤—Å–µ—Ö –≥–ª–∞–≤–∞—Ö –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\"\"\"\n",
    "    total_matches = 0\n",
    "    results = []\n",
    "\n",
    "    for chapter_number, chapter_title, chapter_text in chapters:\n",
    "        matches = search_words_in_chapter(chapter_text, words)\n",
    "        total_matches += len(matches)\n",
    "\n",
    "        for position, match in matches:\n",
    "            results.append([chapter_number, chapter_title, position, match, category])\n",
    "\n",
    "    print(f\"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{category}': –Ω–∞–π–¥–µ–Ω–æ {total_matches} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.\")\n",
    "    return results\n",
    "\n",
    "def split_text_into_chapters(book_text):\n",
    "    \"\"\"–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã. –ü—Ä–∏–º–µ—Ä: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤—É 'Chapter'.\"\"\"\n",
    "    chapters = []\n",
    "    chapter_texts = re.split(r'Chapter', book_text, flags=re.IGNORECASE)\n",
    "    for i, text in enumerate(chapter_texts[1:], start=1):\n",
    "        title_end = text.find('\\n')\n",
    "        chapter_title = text[:title_end].strip()\n",
    "        chapter_content = text[title_end + 1:].strip()\n",
    "        chapters.append((i, chapter_title, chapter_content))\n",
    "    return chapters\n",
    "\n",
    "def find_appearance(doc, path):\n",
    "\n",
    "    print(\"üìñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞...\")\n",
    "\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "        \n",
    "    book_text = '\\n'.join(full_text)\n",
    "\n",
    "    print(\"‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≥–ª–∞–≤—ã...\")\n",
    "    chapters = split_text_into_chapters(book_text)\n",
    "    if not chapters:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –≥–ª–∞–≤—ã.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    all_results = []\n",
    "    for words, category in [\n",
    "        (clothes_words, 'Clothes'), (hair_words, 'Hair'),\n",
    "        (appearances_words, 'Appearances'), (weather_words, 'Weather'),\n",
    "        (locations_words, 'Locations'), (age_words, 'Age'), (other_words, 'Other')\n",
    "    ]:\n",
    "        all_results.extend(search_in_all_chapters(chapters, words, category))\n",
    "\n",
    "    print(\"üìä –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏...\")\n",
    "    all_data_df = pd.DataFrame(all_results, columns=['Chapter Number', 'Chapter Title', 'Position', 'Match', 'Category'])\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–º–µ—Ä—É –≥–ª–∞–≤—ã –∏ –ø–æ–∑–∏—Ü–∏–∏\n",
    "    all_data_df.sort_values(by = ['Chapter Number', 'Position'], inplace = True)\n",
    "    # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ —Å–ª–æ–≤–∞\n",
    "    all_data_df = all_data_df.drop(columns=['Position'])\n",
    "\n",
    "    print(\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel...\")\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    excel_file_path = f\"{path}/Details.xlsx\"\n",
    "\n",
    "    try:\n",
    "        all_data_df.to_excel(excel_file_path, sheet_name='Appearance', index = False)\n",
    "        print(f\"‚úÖ –§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω: {excel_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞: {e}\")\n",
    "\n",
    "##03_clean_summary##\n",
    "def remove_inserts_and_format_headings(file_path, save_path):\n",
    "    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "    doc = Document(file_path)\n",
    "\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å—Ç–∞–≤–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å\n",
    "    patterns = [\n",
    "        r'\\d+o',  # –Ω–∞–ø—Ä–∏–º–µ—Ä, '4o'\n",
    "        r'–í—ã —Å–∫–∞–∑–∞–ª–∏:',  # –Ω–∞–ø—Ä–∏–º–µ—Ä, '–í—ã —Å–∫–∞–∑–∞–ª–∏:'\n",
    "        r'Chapter_\\d+\\.docx',  # –Ω–∞–ø—Ä–∏–º–µ—Ä, 'Chapter_2.docx', 'Chapter_3.docx'\n",
    "        r'–î–æ–∫—É–º–µ–Ω—Ç',  # –Ω–∞–ø—Ä–∏–º–µ—Ä, '–î–æ–∫—É–º–µ–Ω—Ç'\n",
    "        r'ChatGPT',  # –Ω–∞–ø—Ä–∏–º–µ—Ä, 'ChatGPT'\n",
    "        r'^–≠—Ç–æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –Ω–∞—Ä—É—à–∞—Ç—å –Ω–∞—à—É –ø–æ–ª–∏—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\\.$',  # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã\n",
    "        r'^–ü–∞–º—è—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞$'  # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã\n",
    "    ]\n",
    "\n",
    "    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≥–ª–∞–≤: –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≥–ª–∞–≤—ã + –Ω–æ–º–µ—Ä\n",
    "    chapter_heading_pattern = re.compile(r'^–ì–ª–∞–≤–∞\\s+\\d+')\n",
    "\n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –Ω—É–∂–Ω–æ –ª–∏ —É–¥–∞–ª—è—Ç—å –∞–±–∑–∞—Ü\n",
    "    def should_delete(paragraph):\n",
    "        for pattern in patterns:\n",
    "            if re.match(pattern, paragraph.text.strip()):  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –∞–±–∑–∞—Ü —Ü–µ–ª–∏–∫–æ–º —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —à–∞–±–ª–æ–Ω–æ–º\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "    new_doc = Document()\n",
    "\n",
    "    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∞–±–∑–∞—Ü–∞–º –∏ —É–¥–∞–ª—è–µ–º —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —à–∞–±–ª–æ–Ω–∞–º\n",
    "    for para in doc.paragraphs:\n",
    "        if not should_delete(para):\n",
    "            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –≥–ª–∞–≤—ã, –ø—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª—å \"–ó–∞–≥–æ–ª–æ–≤–æ–∫ 3\"\n",
    "            if chapter_heading_pattern.match(para.text.strip()):\n",
    "                new_doc.add_paragraph(para.text, style='Heading 3')\n",
    "            else:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü –≤ –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "                new_para = new_doc.add_paragraph()\n",
    "                for run in para.runs:\n",
    "                    new_run = new_para.add_run(run.text)\n",
    "                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "                    new_run.bold = run.bold\n",
    "                    new_run.italic = run.italic\n",
    "                    new_run.underline = run.underline\n",
    "                    new_run.font.name = run.font.name\n",
    "                    new_run.font.size = run.font.size\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç\n",
    "    new_doc.save(save_path)\n",
    "\n",
    "    # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞\n",
    "    abs_path = os.path.abspath(save_path)\n",
    "    print(f\"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: {abs_path}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def clean_summary(docx_doc, path):\n",
    "    file_path = f\"{path}/summary.docx\"  # –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª\n",
    "    save_path = f\"{path}/summary_clean.docx\"  # –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–µ–∑ –≤—Å—Ç–∞–≤–æ–∫\n",
    "    \n",
    "    remove_inserts_and_format_headings(file_path, save_path)\n",
    "\n",
    "\n",
    "\n",
    "##04_Cover##\n",
    "def extract_sections(doc, summary_label, location_label, character_label):\n",
    "    summaries = []\n",
    "    locations = []\n",
    "    characters = []\n",
    "\n",
    "    current_section = \"\"\n",
    "    section_type = None\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "\n",
    "        if summary_label in text:\n",
    "            if current_section:\n",
    "                if section_type == \"location\":\n",
    "                    locations.append(current_section.strip())\n",
    "                elif section_type == \"character\":\n",
    "                    characters.append(current_section.strip())\n",
    "            section_type = \"summary\"\n",
    "            current_section = \"\"\n",
    "\n",
    "        elif location_label in text:\n",
    "            if current_section:\n",
    "                if section_type == \"summary\":\n",
    "                    summaries.append(current_section.strip())\n",
    "                elif section_type == \"character\":\n",
    "                    characters.append(current_section.strip())\n",
    "            section_type = \"location\"\n",
    "            current_section = \"\"\n",
    "\n",
    "        elif character_label in text:\n",
    "            if current_section:\n",
    "                if section_type == \"summary\":\n",
    "                    summaries.append(current_section.strip())\n",
    "                elif section_type == \"location\":\n",
    "                    locations.append(current_section.strip())\n",
    "            section_type = \"character\"\n",
    "            current_section = \"\"\n",
    "\n",
    "        elif text == \"\":\n",
    "            continue\n",
    "\n",
    "        if section_type == \"summary\":\n",
    "            current_section += text + \" \"\n",
    "        elif section_type == \"location\":\n",
    "            current_section += text + \" \"\n",
    "        elif section_type == \"character\":\n",
    "            current_section += text + \" \"\n",
    "\n",
    "    if current_section:\n",
    "        if section_type == \"summary\":\n",
    "            summaries.append(current_section.strip())\n",
    "        elif section_type == \"location\":\n",
    "            locations.append(current_section.strip())\n",
    "        elif section_type == \"character\":\n",
    "            characters.append(current_section.strip())\n",
    "\n",
    "    return summaries, locations, characters\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª—ã\n",
    "def save_to_docx(data, output_path, title_prefix):\n",
    "    doc = Document()\n",
    "    for idx, section in enumerate(data, start=1):\n",
    "        doc.add_heading(f\"{title_prefix} {idx}\", level=1)\n",
    "        doc.add_paragraph(section)\n",
    "    doc.save(output_path)\n",
    "    print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–ª–µ–Ω–∏—è summaries –Ω–∞ 3 —á–∞—Å—Ç–∏, –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—è –≥–ª–∞–≤—ã\n",
    "def split_summaries_evenly(summaries):\n",
    "    chunk_size = math.ceil(len(summaries) / 3)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    for i in range(3):\n",
    "        end = min(start + chunk_size, len(summaries))\n",
    "        chunks.append(summaries[start:end])\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "def create_cover_info(docx_doc, path):\n",
    "    summary_label = \"–°–æ–±—ã—Ç–∏—è –≥–ª–∞–≤—ã\"\n",
    "    location_label = \"–õ–æ–∫–∞—Ü–∏–∏ –≥–ª–∞–≤—ã\"\n",
    "    character_label = \"–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –≥–ª–∞–≤—ã\"\n",
    "\n",
    "    output_path = path\n",
    "    summary = Document(f\"{output_path}/summary_clean.docx\")\n",
    "    summaries, locations, characters = extract_sections(summary, summary_label, location_label, character_label)\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "    save_to_docx(summaries, f\"{output_path}/synopsis.docx\", \"–°–æ–±—ã—Ç–∏—è –≥–ª–∞–≤—ã\")\n",
    "    save_to_docx(locations, f\"{output_path}/locations.docx\", \"–õ–æ–∫–∞—Ü–∏–∏ –≥–ª–∞–≤—ã\")\n",
    "    save_to_docx(characters, f\"{output_path}/characters.docx\", \"–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –≥–ª–∞–≤—ã\")\n",
    "\n",
    "    # –î–µ–ª–∏–º summaries –Ω–∞ 3 —á–∞—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ö\n",
    "    chunks = split_summaries_evenly(summaries)\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        save_to_docx(chunk, f\"{output_path}/summary_part_{i}.docx\", \"–ì–ª–∞–≤–∞\")\n",
    "\n",
    "    print(\"–§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4402-07b2-42cc-b647-fc6a7e1ca711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
